{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Practice 2: Generating names with a decoder-only transformer\n",
    "\n",
    "In this notebook, we will generate new names using a Transformer decoder model. This simple text generation task captures the essential components of language modeling, applied to a small, manageable dataset.\n",
    "\n",
    "More specifically, you will be training autoregressive, character-level, decoder-only language model. You will feed it a database of names, and the model will generate new name ideas that all sound name-like, but are not already existing names. \n",
    "\n",
    "First, you'll train the model. After training, you'll generate names using pure random sampling as your decoding strategy. Pure random sampling doesn't always work well, so you'll also learn to tweak the temperature parameter when sampling, to better control your generation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import the data processing and model modules\n",
    "from data_processing import load_and_preprocess_data, CharTokenizer, NameDataset, collate_fn\n",
    "from train import train \n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "In this section, you will take a look at the data. You should be familiar with it, but it is used a little different now that we are training a decoder model. Play with it and answer the questions at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "First 10 names: ['-maria carmen.', '-antonio.', '-maria.', '-manuel.', '-jose.']\n"
     ]
    }
   ],
   "source": [
    "data_filepath = \"../data/nombres_raw.txt\"  # Replace with your actual file path\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "start_token = \"-\"\n",
    "end_token = \".\"\n",
    "batch_size = 64\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "names = load_and_preprocess_data(data_filepath, alphabet, start_token, end_token)\n",
    "print(\"First 10 names:\", names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding names...\n",
      "First 10 encoded names: [[1, 16, 4, 21, 12, 4, 3, 6, 4, 21, 16, 8, 17, 2], [1, 4, 17, 23, 18, 17, 12, 18, 2], [1, 16, 4, 21, 12, 4, 2], [1, 16, 4, 17, 24, 8, 15, 2], [1, 13, 18, 22, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = CharTokenizer(alphabet, start_token=start_token, end_token=end_token)\n",
    "\n",
    "# Encode names\n",
    "print(\"Encoding names...\")\n",
    "encoded_names = [tokenizer.encode(name) for name in names]\n",
    "print(\"First 10 encoded names:\", encoded_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the ```CharTokenizer``` to encode your name. What is the result of encoding your name?\n",
    ">>> Write your name\n",
    "\n",
    ">>> Write your name encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in batch:\n",
      "Input: tensor([ 1,  6, 21, 12, 22, 23, 12, 17,  4,  3, 22, 18, 15,  8,  7,  4,  7,  0,\n",
      "         0,  0])\n",
      "Target: tensor([ 6, 21, 12, 22, 23, 12, 17,  4,  3, 22, 18, 15,  8,  7,  4,  7,  2,  0,\n",
      "         0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = NameDataset(encoded_names)\n",
    "\n",
    "# Create data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Obtain a batch from data loader\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "# LetÂ´s check one item from batch\n",
    "print(\"First item in batch:\")\n",
    "print(\"Input:\", batch[0][0])\n",
    "print(\"Target:\", batch[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can you obtain the target tensor from the input? Does this make sense for an autorregressive prediction such as the one of the Decoder-only model?\n",
    ">>> Write your answer here\n",
    "\n",
    "- What is the tensor value for the start token? And for the end token? And for the padding token?\n",
    ">>> Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and tokenizer\n",
    "Here you will train the decoder model. Feel free to change the hyperparameters of the model in the ```model_params``` dictionary. Be careful with your computational resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Starting training...\n",
      "Epoch [1/10], Training Loss: 2.4638\n",
      "Epoch [1/10], Validation Loss: 2.1985\n",
      "Epoch [2/10], Training Loss: 2.0971\n",
      "Epoch [2/10], Validation Loss: 2.0237\n",
      "Epoch [3/10], Training Loss: 1.9315\n",
      "Epoch [3/10], Validation Loss: 1.8707\n",
      "Epoch [4/10], Training Loss: 1.8119\n",
      "Epoch [4/10], Validation Loss: 1.7804\n",
      "Epoch [5/10], Training Loss: 1.7282\n",
      "Epoch [5/10], Validation Loss: 1.7106\n",
      "Epoch [6/10], Training Loss: 1.6693\n",
      "Epoch [6/10], Validation Loss: 1.6660\n",
      "Epoch [7/10], Training Loss: 1.6221\n",
      "Epoch [7/10], Validation Loss: 1.6246\n",
      "Epoch [8/10], Training Loss: 1.5852\n",
      "Epoch [8/10], Validation Loss: 1.5921\n",
      "Epoch [9/10], Training Loss: 1.5507\n",
      "Epoch [9/10], Validation Loss: 1.5619\n",
      "Epoch [10/10], Training Loss: 1.5281\n",
      "Epoch [10/10], Validation Loss: 1.5438\n"
     ]
    }
   ],
   "source": [
    "# Define training hyper parameters\n",
    "model_save_dir = \"runs\"\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "model_params = {\n",
    "    \"d_model\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 128,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"max_position_embeddings\": tokenizer.vocab_size # Do not touch this\n",
    "}\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Call the train function\n",
    "model = train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    model_save_dir=model_save_dir,\n",
    "    model_params=model_params,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Names\n",
    "Now we are ready to generate new names. Fill the function below and start playing around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix: str = \"\",\n",
    "    start_token: str = \"-\",\n",
    "    end_token: str = \".\",\n",
    "    max_length: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a new name using the trained model, optionally starting with a given prefix.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        tokenizer (CharTokenizer): The tokenizer.\n",
    "        prefix (str): Optional prefix string to start the name.\n",
    "        start_token (str): The start token character.\n",
    "        end_token (str): The end token character.\n",
    "        max_length (int): Maximum length of the generated name (excluding prefix length).\n",
    "        temperature (float): Sampling temperature. Higher values increase randomness.\n",
    "        device (torch.device): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_token_id = tokenizer.char2idx[start_token]\n",
    "    end_token_id = tokenizer.char2idx[end_token]\n",
    "\n",
    "    # TODO: Encode the prefix\n",
    "    prefix_ids = tokenizer.encode(prefix)\n",
    "\n",
    "    # TODO: Initialize the input with the start token and the prefix\n",
    "    generated_ids = [start_token_id] + prefix_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # TODO: Get model predictions\n",
    "            logits = model(torch.tensor(generated_ids, device=device).unsqueeze(0))\n",
    "            \n",
    "            # TODO: Apply softmax to get probabilities\n",
    "            next_token_probs = logits.softmax(dim=-1)\n",
    "\n",
    "            # TODO: Sample the next token\n",
    "            next_token_id = torch.multinomial(next_token_probs[:, -1], num_samples=1)\n",
    "\n",
    "            # TODO: Append the new token to the sequence\n",
    "            generated_ids.append(next_token_id.item())\n",
    "\n",
    "            # Stop if end token is generated\n",
    "            if next_token_id.item() == end_token_id:\n",
    "                break\n",
    "\n",
    "    # TODO: Decode the generated token IDs to a string, excluding start and end tokens\n",
    "    generated_sequence = tokenizer.decode(generated_ids[1:-1])\n",
    "\n",
    "    # TODO: Decode the name\n",
    "    generated_name = generated_sequence\n",
    "\n",
    "    return generated_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_names = 5          # Number of names to generate\n",
    "max_length = 20         # Maximum length of each generated name\n",
    "temperature = 1.0       # Sampling temperature \n",
    "start_token = \"-\"       # Start token character (used during training)\n",
    "end_token = \".\"         # End token character (used during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names:\n",
      "\n",
      "sanatza vilerma\n",
      "sivel\n",
      "baret\n",
      "antonia julio\n",
      "alan angelica\n"
     ]
    }
   ],
   "source": [
    "# Generate names\n",
    "print(\"Generated Names:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Temperature Parameter in Language Generation\n",
    "\n",
    "The **temperature** parameter $T$ adjusts the randomness of text generated by language models by scaling the logits (model outputs before softmax).\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Given logits $z_i$ for each token $i$, the probability $p_i$ of selecting token $i$ is calculated using the softmax function:\n",
    "\n",
    "\\begin{align*}\n",
    "p_i = \\frac{\\exp\\left(\\frac{z_i}{T}\\right)}{\\sum_{j} \\exp\\left(\\frac{z_j}{T}\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "- *When $T = 1$*: The probabilities remain unchanged.\n",
    "- *When $T < 1$*: The distribution becomes sharper; higher-probability tokens are favored. You should expect names to look more like the \"typical\" names encountered in the dataset.\n",
    "- *When $T > 1$*: The distribution flattens; lower-probability tokens are more likely. You should expect names to look more \"exotic\" or \"creative\", since less probable characters are being sampled to continue the previously generated ones.\n",
    "\n",
    "### Impact on Token Probabilities\n",
    "\n",
    "Suppose we have logits for three tokens:\n",
    "\n",
    "- $z_A$ = 2.0\n",
    "- $z_B$ = 1.0\n",
    "- $z_C$ = 0.5\n",
    "\n",
    "##### At $T = 1.0$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.659\\\\\n",
    "p_B &= \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.242\\\\\n",
    "p_C &= \\frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.099\n",
    "\\end{align*}\n",
    "\n",
    "##### At $T = 0.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 0.5}}{e^{2.0 / 0.5} + e^{1.0 / 0.5} + e^{0.5 / 0.5}} = \\frac{e^{4.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.843\\\\\n",
    "p_B &= \\frac{e^{2.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.114\\\\\n",
    "p_C &= \\frac{e^{1.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.043\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Lower $T$ increases the dominance of the highest logit.\n",
    "\n",
    "##### At $T = 1.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 1.5}}{e^{2.0 / 1.5} + e^{1.0 / 1.5} + e^{0.5 / 1.5}} = \\frac{e^{1.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.490\\\\\n",
    "p_B &= \\frac{e^{0.667}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.282\\\\\n",
    "p_C &= \\frac{e^{0.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.228\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Higher $T$ increases the probabilities of less likely tokens.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- **Low Temperature ($T < 1$)**:\n",
    "  - **Sharper Distribution**: Model is confident; outputs are more predictable.\n",
    "  - **Use Case**: When coherence is crucial.\n",
    "\n",
    "- **High Temperature ($T > 1$)**:\n",
    "  - **Flatter Distribution**: Model explores more options; outputs are diverse.\n",
    "  - **Use Case**: When creativity is desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names with temperature=1.5:\n",
      "\n",
      "marcos auriud\n",
      "feley derich\n",
      "galona\n",
      "jose humik\n",
      "antonio alejandra\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.5  # High randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Names with temperature=0.75:\n",
      "\n",
      "jesus michaela\n",
      "frunyzna legank\n",
      "maria isabel\n",
      "bouedar\n",
      "gabriela alejandra \n"
     ]
    }
   ],
   "source": [
    "temperature = 0.75  # Low randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Prefix to start the names with\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_names):\n\u001b[0;32m----> 4\u001b[0m     name \u001b[38;5;241m=\u001b[39m generate_name(\n\u001b[1;32m      5\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      6\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m      7\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      8\u001b[0m         start_token\u001b[38;5;241m=\u001b[39mstart_token,\n\u001b[1;32m      9\u001b[0m         end_token\u001b[38;5;241m=\u001b[39mend_token,\n\u001b[1;32m     10\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     11\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     12\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mgenerate_name\u001b[0;34m(model, tokenizer, prefix, start_token, end_token, max_length, temperature, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m end_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mchar2idx[end_token]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO: Encode the prefix\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m prefix_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prefix)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# TODO: Initialize the input with the start token and the prefix\u001b[39;00m\n\u001b[1;32m     35\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m [start_token_id] \u001b[38;5;241m+\u001b[39m prefix_ids\n",
      "File \u001b[0;32m~/Desktop/iMat 4Âº/nlp/p2-decoder-LuchitoGarcia/notebook/../src/data_processing.py:80\u001b[0m, in \u001b[0;36mCharTokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Encode a string into a list of integer token IDs.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m        List[int]: A list of integer token IDs.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar2idx[char] \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Z'"
     ]
    }
   ],
   "source": [
    "temperature = 1.0  # Default randomness\n",
    "prefix = \"Z\"  # Prefix to start the names with\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        prefix=prefix,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
